{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WtRh_Fa5EDce"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tiktoken\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0JFlTI4NDPY_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"instruction-data.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "with open(\"instruction-data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for ex in data:\n",
        "        instr = ex.get(\"instruction\", \"\")\n",
        "        inp = ex.get(\"input\", \"\")\n",
        "        out = ex.get(\"output\", \"\")\n",
        "        f.write(f\"Instruction: {instr}\\nInput: {inp}\\nOutput: {out}\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hkRVmS5xD8XS"
      },
      "outputs": [],
      "source": [
        "class TiktokenDataset(Dataset):\n",
        "    def __init__(self, filepath, tokenizer, block_size):\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            data = f.read()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.block_size = block_size\n",
        "        self.encoded = tokenizer.encode(data)\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(0, len(self.encoded) - self.block_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        chunk = self.encoded[idx:idx+self.block_size+1]\n",
        "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "km3ZZM36EAZ_"
      },
      "outputs": [],
      "source": [
        "class GPTConfig:\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        block_size,\n",
        "        n_layers=4,\n",
        "        n_heads=4,\n",
        "        n_embd=128,\n",
        "        dropout=0.1,\n",
        "        bias=True,\n",
        "        tie_weights=True,\n",
        "        use_learnable_pos_emb=True,\n",
        "    ):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.n_embd = n_embd\n",
        "        self.dropout = dropout\n",
        "        self.bias = bias\n",
        "        self.tie_weights = tie_weights\n",
        "        self.use_learnable_pos_emb = use_learnable_pos_emb\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_heads == 0\n",
        "        self.n_heads = config.n_heads\n",
        "        self.head_dim = config.n_embd // config.n_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        self.proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.attn_drop = nn.Dropout(config.dropout)\n",
        "        self.resid_drop = nn.Dropout(config.dropout)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size)).unsqueeze(0).unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.qkv(x)\n",
        "        q, k, v = qkv.split(C, dim=2)\n",
        "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_drop(att)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_drop(self.proj(y))\n",
        "        return y\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            nn.Dropout(config.dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        if config.use_learnable_pos_emb:\n",
        "            self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
        "        else:\n",
        "            self.register_buffer('pos_emb', self._get_sinusoidal_pos_emb(config.block_size, config.n_embd))\n",
        "        self.drop = nn.Dropout(config.dropout)\n",
        "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=config.bias)\n",
        "        if config.tie_weights:\n",
        "            self.head.weight = self.tok_emb.weight\n",
        "        self.block_size = config.block_size\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def _get_sinusoidal_pos_emb(self, seq_len, dim):\n",
        "        position = torch.arange(seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / dim))\n",
        "        pe = torch.zeros(seq_len, dim)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        return pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.block_size, \"Sequence too long\"\n",
        "        tok_emb = self.tok_emb(idx)\n",
        "        if self.config.use_learnable_pos_emb:\n",
        "            pos_emb = self.pos_emb[:, :T, :]\n",
        "        else:\n",
        "            pos_emb = self.pos_emb[:, :T, :].to(tok_emb.device)\n",
        "        x = self.drop(tok_emb + pos_emb)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddvxkTUBDuoz",
        "outputId": "95f57722-782c-4876-f20b-3d4bd1304129"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Finetune Epoch 1: 100%|██████████| 9906/9906 [17:23<00:00,  9.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finetune Epoch 1, Loss: 4.0744\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Finetune Epoch 2:  95%|█████████▍| 9394/9906 [16:20<00:49, 10.43it/s]"
          ]
        }
      ],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "block_size = 32\n",
        "config = GPTConfig(\n",
        "    vocab_size=tokenizer.n_vocab,\n",
        "    block_size=block_size,\n",
        "    n_layers=1,\n",
        "    n_heads=1,\n",
        "    n_embd=32,\n",
        "    dropout=0.1,\n",
        "    tie_weights=True,\n",
        "    use_learnable_pos_emb=True,\n",
        ")\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Loading pretrained model\n",
        "model = GPT(config).to(device)\n",
        "model.load_state_dict(torch.load('gpt_tiktoken.pth', map_location=device))\n",
        "\n",
        "# Preparing finetuning dataset\n",
        "finetune_dataset = TiktokenDataset(\"instruction-data.txt\", tokenizer, block_size)\n",
        "finetune_loader = DataLoader(finetune_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
        "\n",
        "# Finetuning the model\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "epochs = 2\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x, y in tqdm(finetune_loader, desc=f\"Finetune Epoch {epoch+1}\"):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits, loss = model(x, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(finetune_loader)\n",
        "    print(f\"Finetune Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Saving the finetuned model\n",
        "torch.save(model.state_dict(), \"gpt_tiktoken_finetuned.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRIsyR61NRws",
        "outputId": "bfc2fbdb-9004-400d-f517-7800c5717fba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Instruction: Write a short poem about the sea.\n",
            "Input: \n",
            "Output: A more seen the statement using a sentence in the sentence using the sentence.\n",
            "Input: \n",
            "Output: The quick brown innovative\" is veryNO3.\n",
            "\n",
            "Output: \n",
            "Instruction: The formula for the plural form of one of\n"
          ]
        }
      ],
      "source": [
        "def sample(model, tokenizer, start_text, length=50, temperature=1.0):\n",
        "    model.eval()\n",
        "    idx = torch.tensor([tokenizer.encode(start_text)], dtype=torch.long).to(next(model.parameters()).device)\n",
        "    for _ in range(length):\n",
        "        idx_cond = idx[:, -model.config.block_size:]\n",
        "        logits, _ = model(idx_cond)\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        next_id = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat([idx, next_id], dim=1)\n",
        "    return tokenizer.decode(idx[0].tolist())\n",
        "\n",
        "prompt = \"Instruction: Write a short poem about the sea.\\nInput: \\nOutput:\"\n",
        "print(sample(model, tokenizer, prompt, length=50))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
